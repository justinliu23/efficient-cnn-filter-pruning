{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQGm6JtgehIq"
      },
      "source": [
        "> CNN Filter Pruning Project for Harvard CS 2420: Computing at Scale (Fall 2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### **Setup**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "BXId-u1Vk_aP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRz6CSs4LwBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4bcf93-d148-47a2-fba5-c8637d87ad67"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 10 01:13:25 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0              46W / 400W |   1079MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9WL6HA_Lpe8"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tracks the highest accuracy observed so far\n",
        "best_acc = 0\n",
        "\n",
        "def moving_average(a, n=100):\n",
        "    '''Helper function used for visualization'''\n",
        "    ret = torch.cumsum(torch.Tensor(a), 0)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "def train(net, epoch, loader, criterion, optimizer, loss_tracker = [], acc_tracker = []):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        # update optimizer state\n",
        "        optimizer.step()\n",
        "        # compute average loss\n",
        "        train_loss += loss.item()\n",
        "        loss_tracker.append(loss.item())\n",
        "        loss = train_loss / (batch_idx + 1)\n",
        "        # compute accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        acc = 100. * correct / total\n",
        "        # Print status\n",
        "        sys.stdout.write(f'\\rEpoch {epoch}: Train Loss: {loss:.3f}' +\n",
        "                         f'| Train Acc: {acc:.3f}')\n",
        "        sys.stdout.flush()\n",
        "    acc_tracker.append(acc)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def test(net, epoch, loader, criterion, loss_tracker = [], acc_tracker = []):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            loss_tracker.append(loss.item())\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            loss = test_loss / (batch_idx + 1)\n",
        "            acc = 100.* correct / total\n",
        "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # Save checkpoint\n",
        "    acc = 100.*correct/total\n",
        "    acc_tracker.append(acc)\n",
        "    if acc > best_acc:\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc"
      ],
      "metadata": {
        "id": "BeYR2zLEM1BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOTGlfOG5IAi"
      },
      "source": [
        "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
        "               padding=1):\n",
        "    '''\n",
        "    A nn.Sequential layer executes its arguments in sequential order. In\n",
        "    this case, it performs Conv2d -> BatchNorm2d -> ReLU. This is a typical\n",
        "    block of layers used in Convolutional Neural Networks (CNNs). The\n",
        "    ConvNet implementation below stacks multiple instances of this three layer\n",
        "    pattern in order to achieve over 90% classification accuracy on CIFAR-10.\n",
        "    '''\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
        "                  bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    '''\n",
        "    A 9 layer CNN using the conv_block function above. Again, we use a\n",
        "    nn.Sequential layer to build the entire model. The Conv2d layers get\n",
        "    progressively larger (more filters) as the model gets deeper. This\n",
        "    corresponds to spatial resolution getting smaller (via the stride=2 blocks),\n",
        "    going from 32x32 -> 16x16 -> 8x8. The nn.AdaptiveAvgPool2d layer at the end\n",
        "    of the model reduces the spatial resolution from 8x8 to 1x1 using a simple\n",
        "    average across all the pixels in each channel. This is then fed to the\n",
        "    single fully connected (linear) layer called classifier, which is the output\n",
        "    prediction of the model.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(3, 32),\n",
        "            conv_block(32, 32),\n",
        "            conv_block(32, 64, stride=2),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 128, stride=2),\n",
        "            conv_block(128, 128),\n",
        "            conv_block(128, 256),\n",
        "            conv_block(256, 256),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "            )\n",
        "\n",
        "        self.classifier = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        The forward function is called automatically by the model when it is\n",
        "        given an input image. It first applies the 8 convolution layers, then\n",
        "        finally the single classifier layer.\n",
        "        '''\n",
        "        h = self.model(x)\n",
        "        B, C, _, _ = h.shape\n",
        "        h = h.view(B, C)\n",
        "        return self.classifier(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myU7s113oeLS"
      },
      "source": [
        "---\n",
        "\n",
        "### **Structured and Non-structured Filter Pruning**\n",
        "\n",
        "---\n",
        "\n",
        "In this section, we will implement a simplified version of structured filter pruning proposed in [Pruning Filters for Efficient ConvNets](https://openreview.net/pdf?id=rJqFGTslg). Instead of pruning weights, this paper describes removing whole filters from each convolutional layer in a CNN. Compared to pruning weights across the network, filter pruning is a naturally structured pruning method that does not introduce irregular sparsity. Therefore, it does not require using sparse libraries or specialized hardware.\n",
        "For each convolutional layer, we measure each filter’s relative importance by its absolute weight sum $\\sum|\\mathcal{F}_{i,j}|$ (i.e., its $\\ell_1$-norm). When pruning a layer, $m$ filters with the smallest relative importance will be pruned, where $m$ = (prune percentage $\\times$ total number of filters in this layer).\n",
        "\n",
        "Besides structured pruning, we will also implement non-structured pruning proposed in [Learning both Weights and Connections for Efficient Neural Networks\n",
        "](https://arxiv.org/abs/1506.02626) for comparsion. Non-structured pruning is more flexible than structured pruning, and allows irregular sparsity in the weight tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76ay4wFg7vWZ"
      },
      "source": [
        "def _make_pair(x):\n",
        "    if hasattr(x, '__len__'):\n",
        "        return x\n",
        "    else:\n",
        "        return (x, x)\n",
        "\n",
        "class SparseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                     padding=1):\n",
        "        super(SparseConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = _make_pair(stride)\n",
        "        self.padding = _make_pair(padding)\n",
        "\n",
        "        # initialize weights of this layer\n",
        "        self._weight = nn.Parameter(torch.randn([self.out_channels, self.in_channels,\n",
        "                                                        self.kernel_size, self.kernel_size]))\n",
        "        stdv = 1. / math.sqrt(in_channels)\n",
        "        self._weight.data.uniform_(-stdv, stdv)\n",
        "        # initialize mask\n",
        "        # Since we are going to zero out the whole filter, the number of\n",
        "        # elements in the mask is equal to the number of filters.\n",
        "        self.register_buffer('_mask', torch.ones(out_channels))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(x, self.weight, stride=self.stride,\n",
        "                        padding=self.padding)\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self._mask[:,None,None,None] * self._weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvtFRYWY8z7O"
      },
      "source": [
        "def sparse_conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                      padding=1):\n",
        "    '''\n",
        "    Replaces 3x3 nn.Conv2d with 3x3 SparseConv2d\n",
        "    '''\n",
        "    return nn.Sequential(\n",
        "        SparseConv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "class SparseConvNet(nn.Module):\n",
        "    '''\n",
        "    A 9 layer CNN using the sparse_conv_block function above.\n",
        "    PART 3.1: Implement!\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(SparseConvNet, self).__init__()\n",
        "        print(\"Hello from constructor\")\n",
        "\n",
        "        ## We simply create a sequence of sparse convolution blocks\n",
        "        self.model = nn.Sequential(\n",
        "            sparse_conv_block(3, 32),\n",
        "            sparse_conv_block(32, 32),\n",
        "            sparse_conv_block(32, 64, stride=2),\n",
        "            sparse_conv_block(64, 64),\n",
        "            sparse_conv_block(64, 64),\n",
        "            sparse_conv_block(64, 128, stride=2),\n",
        "            sparse_conv_block(128, 128),\n",
        "            sparse_conv_block(128, 256),\n",
        "            sparse_conv_block(256, 256),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        PART 3.1: Implement!\n",
        "        '''\n",
        "\n",
        "        h = self.model(x)\n",
        "        B, C, _, _ = h.shape\n",
        "        h = h.view(B, C)\n",
        "        return self.classifier(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tstv8XasCR02"
      },
      "source": [
        "torch.manual_seed(43) # to give stable randomness\n",
        "\n",
        "def get_sparse_conv2d_layers(net):\n",
        "    '''\n",
        "    Helper function which returns all SparseConv2d layers in the net.\n",
        "    Use this below to implement layerwise pruning.\n",
        "    '''\n",
        "    sparse_conv_layers = []\n",
        "    for layer in net.children():\n",
        "        if isinstance(layer, SparseConv2d):\n",
        "            sparse_conv_layers.append(layer)\n",
        "        else:\n",
        "            child_layers = get_sparse_conv2d_layers(layer)\n",
        "            sparse_conv_layers.extend(child_layers)\n",
        "\n",
        "    return sparse_conv_layers\n",
        "\n",
        "def filter_l1_pruning(net, prune_percent):\n",
        "    for i, layer in enumerate(get_sparse_conv2d_layers(net)):\n",
        "        num_nonzero = int(layer._mask.sum().item())\n",
        "        num_total = len(layer._mask)\n",
        "        num_prune = round(num_total * prune_percent)\n",
        "        sparsity = 100.0 * (1 - (num_nonzero / num_total))\n",
        "#         print(num_prune, num_total, num_nonzero, prune_percent)\n",
        "\n",
        "        ## We keep track of filters' L1 norms, as well as which filter they are\n",
        "        L1_norms = []\n",
        "        for j, filter in enumerate(layer._weight):\n",
        "\n",
        "            ## Don't want to re-mask something that's been pruned\n",
        "            if layer._mask.data[j] != 0:\n",
        "                L1_norms.append((filter.norm(p = 1), j))\n",
        "\n",
        "        ## Bring the filters with lowest norm to the front\n",
        "        L1_norms.sort()\n",
        "\n",
        "        ## The number of additional filters we need to prune to reach the desired amount\n",
        "        for j in range(num_prune - (num_total - num_nonzero)):\n",
        "            layer._mask.data[L1_norms[j][1]] = 0\n",
        "\n",
        "device = 'cuda'\n",
        "net = SparseConvNet()\n",
        "net = net.to(device)\n",
        "\n",
        "lr = 0.1\n",
        "milestones = [24, 49, 74, 99]\n",
        "\n",
        "## We change to a list\n",
        "prune_percentages = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "prune_epochs = [10, 20, 30, 40, 50]\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
        "                            weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                 milestones=milestones,\n",
        "                                                 gamma=0.1)\n",
        "\n",
        "train_loss_tracker, train_acc_tracker = [], []\n",
        "test_loss_tracker, test_acc_tracker = [], []\n",
        "\n",
        "print('Training for {} epochs, with learning rate {} and milestones {}'.format(\n",
        "      epochs, lr, milestones))\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(0, epochs):\n",
        "    train(net=net, epoch=epoch, loader=trainloader, criterion=criterion, optimizer=optimizer, loss_tracker=train_loss_tracker, acc_tracker=train_acc_tracker)\n",
        "\n",
        "    if epoch in prune_epochs:\n",
        "        idx = prune_epochs.index(epoch)\n",
        "        prune_epoch = prune_epochs[idx]\n",
        "        prune_percentage = prune_percentages[idx]\n",
        "        print('Pruning at epoch {}'.format(epoch))\n",
        "        filter_l1_pruning(net, prune_percentage)\n",
        "        # unstructured_pruning(net, prune_percentage)\n",
        "\n",
        "    test(net=net, epoch=epoch, loader=testloader, criterion=criterion, loss_tracker=test_loss_tracker, acc_tracker=test_acc_tracker)\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SparseConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                     padding=1):\n",
        "        super(SparseConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = _make_pair(stride)\n",
        "        self.padding = _make_pair(padding)\n",
        "\n",
        "        # initialize weights of this layer\n",
        "        self._weight = nn.Parameter(torch.randn([self.out_channels, self.in_channels,\n",
        "                                                        self.kernel_size, self.kernel_size]))\n",
        "        stdv = 1. / math.sqrt(in_channels)\n",
        "        self._weight.data.uniform_(-stdv, stdv)\n",
        "        # initialize mask\n",
        "        # Since we are going to zero out the whole filter, the number of\n",
        "        # elements in the mask is equal to the number of filters.\n",
        "        self.register_buffer('_mask', torch.ones_like(self._weight))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.conv2d(x, self.weight, stride=self.stride,\n",
        "                        padding=self.padding)\n",
        "\n",
        "    @property\n",
        "    def weight(self):\n",
        "        return self._mask * self._weight\n",
        "\n",
        "def unstructured_pruning(net, prune_percent):\n",
        "    for i, layer in enumerate(get_sparse_conv2d_layers(net)):\n",
        "        num_nonzero = layer._mask.sum().item()\n",
        "        num_total = layer._mask.numel()\n",
        "        num_prune = round(num_total * prune_percent)\n",
        "\n",
        "        # Get the absolute value of all weights in the layer\n",
        "        L1_norms = torch.abs(layer._weight)\n",
        "\n",
        "        weight_1d = layer._weight.view(-1)\n",
        "        mask_1d = layer._mask.view(-1)\n",
        "        L1_norms_1d = L1_norms.view(-1)\n",
        "\n",
        "        prune_indices = torch.argsort(L1_norms_1d)\n",
        "        prune_indices = prune_indices[mask_1d[prune_indices] != 0][:int(num_prune - (num_total - num_nonzero))]\n",
        "\n",
        "        mask_1d[prune_indices] = 0\n",
        "        layer._mask.data = mask_1d.view_as(layer._weight)"
      ],
      "metadata": {
        "id": "5jbU5kiBvLxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                      padding=1):\n",
        "    '''\n",
        "    Replaces 3x3 nn.Conv2d with 3x3 SparseConv2d\n",
        "    '''\n",
        "    return nn.Sequential(\n",
        "        SparseConv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "class SparseConvNet(nn.Module):\n",
        "    '''\n",
        "    A 9 layer CNN using the sparse_conv_block function above.\n",
        "    PART 3.1: Implement!\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(SparseConvNet, self).__init__()\n",
        "\n",
        "        ## We simply create a sequence of sparse convolution blocks\n",
        "        self.model = nn.Sequential(\n",
        "            sparse_conv_block(3, 32),\n",
        "            sparse_conv_block(32, 32),\n",
        "            sparse_conv_block(32, 64, stride=2),\n",
        "            sparse_conv_block(64, 64),\n",
        "            sparse_conv_block(64, 64),\n",
        "            sparse_conv_block(64, 128, stride=2),\n",
        "            sparse_conv_block(128, 128),\n",
        "            sparse_conv_block(128, 256),\n",
        "            sparse_conv_block(256, 256),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        PART 3.1: Implement!\n",
        "        '''\n",
        "\n",
        "        h = self.model(x)\n",
        "        B, C, _, _ = h.shape\n",
        "        h = h.view(B, C)\n",
        "        return self.classifier(h)"
      ],
      "metadata": {
        "id": "0geG6ZP1wxtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(43) # to give stable randomness\n",
        "\n",
        "def get_sparse_conv2d_layers(net):\n",
        "    '''\n",
        "    Helper function which returns all SparseConv2d layers in the net.\n",
        "    Use this below to implement layerwise pruning.\n",
        "    '''\n",
        "    sparse_conv_layers = []\n",
        "    for layer in net.children():\n",
        "        if isinstance(layer, SparseConv2d):\n",
        "            sparse_conv_layers.append(layer)\n",
        "        else:\n",
        "            child_layers = get_sparse_conv2d_layers(layer)\n",
        "            sparse_conv_layers.extend(child_layers)\n",
        "\n",
        "    return sparse_conv_layers\n",
        "\n",
        "device = 'cuda'\n",
        "net = SparseConvNet()\n",
        "net = net.to(device)\n",
        "\n",
        "lr = 0.1\n",
        "milestones = [24, 49, 74, 99]\n",
        "\n",
        "## We change to a list\n",
        "prune_percentages = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "prune_epochs = [10, 20, 30, 40, 50]\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
        "                            weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                 milestones=milestones,\n",
        "                                                 gamma=0.1)\n",
        "\n",
        "train_loss_tracker, train_acc_tracker = [], []\n",
        "test_loss_tracker, test_acc_tracker = [], []\n",
        "\n",
        "print('Training for {} epochs, with learning rate {} and milestones {}'.format(\n",
        "      epochs, lr, milestones))\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(0, epochs):\n",
        "    train(net=net, epoch=epoch, loader=trainloader, criterion=criterion, optimizer=optimizer, loss_tracker=train_loss_tracker, acc_tracker=train_acc_tracker)\n",
        "\n",
        "    if epoch in prune_epochs:\n",
        "        idx = prune_epochs.index(epoch)\n",
        "        prune_epoch = prune_epochs[idx]\n",
        "        prune_percentage = prune_percentages[idx]\n",
        "        print('Pruning at epoch {}'.format(epoch))\n",
        "#         filter_l1_pruning(net, prune_percentage)\n",
        "        unstructured_pruning(net, prune_percentage)\n",
        "\n",
        "    test(net=net, epoch=epoch, loader=testloader, criterion=criterion, loss_tracker=test_loss_tracker, acc_tracker=test_acc_tracker)\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "metadata": {
        "id": "afTFPT5xwxiP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}